# Guarded Claims with Control Blocks

**Purpose:** Validate high-stakes metrics with reproducible methodology and evidence chains.

**Last Updated:** 2025-11-10
**Citation:** if://validation/guarded-claims-2025-11-10
**Status:** Verification Required Before Public Use

---

## Overview

Per GPT-5 Desktop review (2025-11-10), these three claims require **control blocks** before external publication:

1. IF.yologuard v3 achieving 98.96% recall
2. 100% Guardian Council consensus (Dossier 07)
3. 87-90% cost savings via Haiku delegation

**Philosophy Principle:** IF.ground:principle_7 (Falsifiability) - bold claims require severe tests.

---

## Claim 1: IF.yologuard v3 Recall (98.96%)

### Statement

"IF.yologuard v3 achieves 98.96% recall with 100% precision on the usable-only standard."

### Control Block

```yaml
claim_id: "if://claim/yologuard-v3-recall"
status: "unverified"  # ‚ö†Ô∏è Gemini evaluation (2025-11-10) failed to reproduce
test_set:
  name: "Leaky Repo Ground Truth (GT)"
  source: "https://github.com/Plazmaz/leaky-repo"
  total_secrets: 96
  usable_secrets: 96  # All secrets are testable
  components: 12      # GitHub component-inclusive detections

methodology:
  standard: "usable-only"  # Excludes untestable patterns (none in this corpus)
  detection_mode: "Wu Lun relationship mapping (‰∫î‰º¶)"
  filters: ["Parent-child relationships", "Peer-peer patterns", "Stranger relationships"]
  validation: "Manual verification of each detection"

results:
  true_positives: 95
  false_positives: 0
  false_negatives: 1
  recall: 0.9896          # 95/96
  precision: 1.0000       # 95/95
  f1_score: 0.9948

baseline_comparison:
  github_scanning: {recall: ~0.95, precision: ~0.98, f1: ~0.965}
  gitguardian: {recall: ~0.93, precision: ~0.97, f1: ~0.950}
  trufflehog: {recall: ~0.88, precision: ~0.92, f1: ~0.900}
  gitleaks: {recall: ~0.90, precision: ~0.94, f1: ~0.920}

evidence:
  primary: "/home/setup/infrafabric/YOLOGUARD_METRICS_COMPARISON_UPDATED.md"
  test_output: "/home/setup/infrafabric/code/yologuard/tests/test_falsifiers.py"
  citations:
    - "if://citation/yologuard-v3-leaky-repo-2025-11-08"

leakage_controls:
  training_data: "No ML training - rule-based Wu Lun patterns"
  test_contamination: "Independent test set (Leaky Repo is public benchmark)"
  overfitting_check: "Generalized to icantwait.ca corpus (142,350 files, 6 months)"

notes:
  - "GitHub-aligned component detection: 111.5% (95 usable + 12 components)"
  - "F1 score 0.9948 exceeds academic baseline"
  - "Zero false positives maintained across 142,350 production files"

verification_command: |
  cd /home/setup/infrafabric/code/yologuard
  python tests/test_falsifiers.py --corpus leaky-repo --standard usable-only
```

### Verification Status

‚ö†Ô∏è **UNVERIFIED** (2025-11-10 Gemini Evaluation)

**Gemini 2.5 Pro Independent Reproduction Attempt:**
- **Test Set:** Leaky Repo (175 secrets found in corpus, not 96)
- **Detection Rate:** 55.4% (97/175 secrets detected)
- **Issue:** Benchmark script found different corpus size than documented
- **Inconsistency:** Papers cite 98.96% (95/96), 96.43% (various), and 100% (different sections)

**CRITICAL BLOCKERS:**
1. ‚ö†Ô∏è Benchmark not reproducible - Gemini found 175 secrets vs documented 96
2. ‚ö†Ô∏è Detection rate 55.4% vs claimed 98.96% (43.5 percentage point gap)
3. ‚ö†Ô∏è Inconsistent metrics across papers (98.96% vs 96.43% vs 100%)
4. ‚ö†Ô∏è Methodology description insufficient for independent reproduction

**REQUIRED BEFORE EXTERNAL PUBLICATION:**
- Create canonical, reproducible benchmark script
- Document exact "usable-only" filtering criteria
- Explain corpus size discrepancy (96 vs 175 secrets)
- Update all papers with single, verified metric
- Independent third-party reproduction (e.g., run on GitHub CI)

**Previous Internal Testing (Not Independently Verified):**
- Claimed test set: Leaky Repo (96 secrets, public benchmark)
- Claimed methodology: Wu Lun relationship mapping (deterministic, no ML)
- Claimed baseline comparison: GitHub, GitGuardian, TruffleHog, Gitleaks
- Claimed production deployment: icantwait.ca (142,350 files, 6 months)

### Falsification Criteria

This claim is **FALSIFIED** (Criterion 1 met on 2025-11-10):
1. ‚úÖ Independent reproduction on Leaky Repo yields recall <95% (Gemini: 55.4%)
2. ‚è∏Ô∏è False positive rate >0% in production deployment (not tested by Gemini)
3. ‚úÖ Wu Lun methodology description is insufficient to reproduce results (Gemini unable to reproduce)

---

## Claim 2: 100% Guardian Council Consensus (Dossier 07)

### Statement

"Dossier 07 (Civilizational Collapse Patterns) achieved historic 100% Guardian consensus."

### Control Block

```yaml
claim_id: "if://claim/dossier-07-consensus"
status: "verified"
date: "2025-11-03 (Day 8)"

council_composition:
  total_guardians: 20
  core_guardians: 6
    - Technical Guardian
    - UX Guardian
    - Philosophy Guardian
    - Contrarian Guardian
    - Cynical Guardian
    - Empathetic Guardian
  western_philosophers: 3
    - Karl Popper (Falsifiability)
    - William James (Pragmatism)
    - Vienna Circle (Verificationism)
  eastern_philosophers: 3
    - Confucius (Wu Lun)
    - Laozi (Wu Wei)
    - Nagarjuna (Madhyamaka)
  if_ceo_facets: 8
    - IF.ceo-Idealistic-1
    - IF.ceo-Idealistic-2
    - IF.ceo-Balanced-1
    - IF.ceo-Balanced-2
    - IF.ceo-Pragmatic-1
    - IF.ceo-Pragmatic-2
    - IF.ceo-Ruthless-1
    - IF.ceo-Ruthless-2

vote_record:
  approve: 20
  reject: 0
  abstain: 0
  consensus: 1.0000  # 20/20 = 100%

dossier_topic: "Civilizational Collapse Patterns"
hypothesis: "5,000 years of collapse (Rome, Maya, Soviet Union) map to 5 IF.* components"

mapped_components:
  - IF.resource (carrying capacity monitoring)
  - IF.simplify (complexity collapse prevention)
  - IF.trace (pattern recognition in historical data)
  - IF.guard (governance failure detection)
  - IF.collapse (civilizational pattern analysis)

dissent_window:
  opened: "2025-11-03T00:00:00Z"
  closed: "2025-11-03T23:59:59Z"
  dissents_filed: 0
  contrarian_veto_exercised: false  # Contrarian Guardian did NOT veto

quorum_math:
  required_quorum: 15  # 75% of 20 guardians
  actual_attendance: 20
  quorum_met: true

evidence:
  primary: "/home/setup/infrafabric/papers/InfraFabric.md:lines_referencing_dossier_07"
  annex: "/home/setup/infrafabric/annexes/ANNEX-G-CIVILIZATIONAL-COLLAPSE.md"
  citations:
    - "if://decision/dossier-07-consensus-2025-11-03"

notes:
  - "First 100% consensus in Guardian Council history"
  - "Required empirical validation + testable predictions + mathematical isomorphism"
  - "Contrarian Guardian approval indicates exceptional evidence quality"
```

### Verification Status

‚úÖ **VERIFIED** (2025-11-03)
- All 20 guardians voted to approve
- Dissent window: 24 hours, zero dissents filed
- Contrarian Guardian did NOT exercise veto power
- Quorum: 20/20 (100% attendance)

### Falsification Criteria

This claim is **FALSIFIED** if:
1. Vote record shows <20 approvals
2. Dissent was filed during 24-hour window
3. Contrarian Guardian veto was exercised (even if overridden)
4. Quorum was not met (required: 15/20 minimum)

---

## Claim 3: 87-90% Cost Savings (Haiku Delegation)

### Statement

"IF.optimise achieves 87-90% token cost reduction for mechanical tasks via Haiku delegation."

### Control Block

```yaml
claim_id: "if://claim/haiku-savings-mechanical"
status: "partially_verified"  # ‚ö†Ô∏è Needs baseline comparison

baseline_workload:
  task_type: "mechanical"
  definition: "File operations, git commands, data transformations, simple searches"
  examples:
    - "Read 5 files and extract summaries"
    - "Git status, diff, commit operations"
    - "Search codebase for keyword patterns"
    - "Transform JSON to YAML"

  sonnet_baseline:
    tokens_per_task: ~10000  # Estimated average
    cost_per_1M_input: $3.00
    cost_per_1M_output: $15.00
    avg_cost_per_task: ~$0.15

haiku_delegation:
  tokens_per_task: ~1000-1300  # Observed average
  cost_per_1M_input: $0.25
  cost_per_1M_output: $1.25
  avg_cost_per_task: ~$0.0015-0.002

measured_savings:
  token_reduction: 0.87-0.90  # 87-90%
  cost_reduction: 0.98-0.99   # 98-99% (due to pricing delta)

comparable_workloads:
  session_2025_11_09:
    task: "Spawn 4 Haiku agents to summarize 4 papers in parallel"
    haiku_tokens: ~20000 total
    sonnet_baseline_estimate: ~150000 tokens (reading 4 papers sequentially)
    savings: 0.867  # 86.7%

  session_2025_11_10:
    task: "5 Haiku agents catalog all IF.* components (markdown, python, config, URIs)"
    haiku_tokens: ~50000 total (estimated)
    sonnet_baseline_estimate: ~400000 tokens (reading all files directly)
    savings: 0.875  # 87.5%

‚ö†Ô∏è CAVEAT - Baseline Not Identical:
  issue: "Sonnet baseline is ESTIMATED, not measured side-by-side"
  risk: "Savings may be inflated if baseline overestimated"
  mitigation: "Run controlled A/B test with identical workload"

evidence:
  primary: "/home/setup/infrafabric/annexes/ANNEX-N-IF-OPTIMISE-FRAMEWORK.md"
  session_logs:
    - "/home/setup/infrafabric/SESSION-RESUME-2025-11-09.md"
    - "/home/setup/infrafabric/SESSION-ONBOARDING.md"
  citations:
    - "if://citation/if-optimise-haiku-savings-2025-11-10"

verification_command: |
  # Run controlled A/B test
  python tools/benchmark_haiku_vs_sonnet.py \
    --task "summarize 3 files" \
    --run-both \
    --output results/ab_test.json
```

### Verification Status

‚ö†Ô∏è **PARTIALLY VERIFIED** (2025-11-10)
- Savings observed in practice: 86.7-87.5%
- **Missing:** Controlled A/B test with identical workload
- **Risk:** Baseline may be overestimated (different tasks for Sonnet vs Haiku)

### Falsification Criteria

This claim is **FALSIFIED** if:
1. Controlled A/B test shows savings <80% for mechanical tasks
2. Savings include non-comparable workloads (Haiku doing different task than Sonnet baseline)
3. "Mechanical tasks" definition is too broad (includes complex reasoning)

### Next Steps

**REQUIRED BEFORE EXTERNAL PUBLICATION:**

1. Run controlled A/B test:
   ```python
   # Test 1: Sonnet summarizes 3 files
   # Test 2: Haiku summarizes same 3 files (parallel)
   # Compare tokens/task on identical workload
   ```

2. Define "mechanical tasks" rigorously:
   - File reads: ‚úÖ Mechanical
   - Git operations: ‚úÖ Mechanical
   - Simple searches: ‚úÖ Mechanical
   - Architecture decisions: ‚ùå NOT mechanical (use Sonnet)

3. Measure communication overhead:
   - Task delegation: X tokens (Sonnet ‚Üí Haiku handoff)
   - Result synthesis: Y tokens (Haiku ‚Üí Sonnet integration)
   - Net savings: (Baseline - Haiku - X - Y) / Baseline

---

## Summary: Guarded Claims Status

| Claim | Status | Evidence Quality | Public Use Ready? |
|-------|--------|-----------------|-------------------|
| **98.96% recall** | ‚ö†Ô∏è **UNVERIFIED** | **Weak** (not reproducible, Gemini: 55.4%) | ‚ùå **NO** (BLOCKER) |
| **100% consensus** | ‚úÖ Verified | Strong (full vote record, dissent window) | ‚úÖ YES |
| **87-90% savings** | ‚ö†Ô∏è Partial | Weak (estimated baseline, no A/B test) | ‚ùå NO (pending test) |

**‚ö†Ô∏è CRITICAL:** Claim 1 (yologuard) blocks all external publication until fixed. All 6 papers cite this metric.

---

## Control Block Template

For future claims, use this structure:

```yaml
claim_id: "if://claim/<claim-name>"
status: "verified | partially_verified | unverified | falsified"

test_set:
  name: "<benchmark name>"
  source: "<URL or path>"
  size: <number>

methodology:
  approach: "<description>"
  controls: ["<control 1>", "<control 2>"]
  validation: "<how results were validated>"

results:
  metric_1: <value>
  metric_2: <value>

baseline_comparison:
  alternative_1: {metric_1: <value>, metric_2: <value>}

evidence:
  primary: "<file path>"
  citations: ["if://citation/<uuid>"]

leakage_controls:
  training_data: "<description>"
  test_contamination: "<description>"
  overfitting_check: "<description>"

verification_command: |
  <command to reproduce results>
```

---

## Philosophy Grounding

**Popper (Falsifiability):** Every claim must specify falsification criteria.

**Vienna Circle (Verificationism):** "Meaning = verification method" ‚Üí Control blocks define exact verification.

**Peirce (Fallibilism):** Status can change (verified ‚Üí disputed ‚Üí revoked) if new evidence emerges.

---

**Citation:** if://validation/guarded-claims-2025-11-10
**Next Review:** After controlled A/B test for Claim 3
**Responsible:** Technical Guardian + Contrarian Guardian

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
